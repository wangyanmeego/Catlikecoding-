

# 02-着色器基础

原文链接：https://catlikecoding.com/unity/tutorials/rendering/part-2/

> 顶点变换
> 给像素赋颜色
> 使用着色器属性
> 将数据从顶点着色器传递到片段着色器
> 查看编译的着色器代码
> 用平铺和偏移对纹理采样

这是渲染教程系列的第二部分。第一部分是关于矩阵的。这次我们将编写第一个着色器并导入一个纹理。
本教程是使用Unity 5.4.0b10

![给一个球贴纹理](https://catlikecoding.com/unity/tutorials/rendering/part-2/tutorial-image.png)

## 默认场景

当你在Unity中创建一个新场景时，你会默认得到一个相机和一个直射灯光  。通过GameObject / 3D Object / sphere创建一个简单的球体，把它放在原点，把相机放在它的前面。

![默认场景中的默认球体](https://catlikecoding.com/unity/tutorials/rendering/part-2/default-scene/default-spere.png)

这是一个非常简单的场景，但是已经存在很多复杂的渲染在进行着了。为了很好地把握渲染的过程，我们要摆脱所有花哨的东西而只关心我们的基本原理。

### 把影响你的东西都剥离掉

通过Window/Lighting查看场景的灯光设置。这将会打开一个有三个选项的灯光窗口。我们只对Scene选项感兴趣，打开的时候默认是选择它的。

![默认灯光设置](https://catlikecoding.com/unity/tutorials/rendering/part-2/default-scene/default-lighting.png)

这是关于环境光照的部分，你可以选择一个天空盒。这个天空盒目前用于场景背景、环境照明和反射。将它设置为None，这样它就被关闭了。
同时，你也可以禁用预计算实时全局光照。我们短期内不会用到这些。

![关闭天空盒](https://catlikecoding.com/unity/tutorials/rendering/part-2/default-scene/simplified-lighting.png)

如果没有天空盒，环境光源自动切换为纯色。默认的颜色是深空灰色和非常淡的蓝色。反射会变成纯黑色，就像警告框所示的那样。
如你所料，球体会变得更暗，背景现在是纯色的。然而，背景是深蓝色的。这种颜色从哪里来的呢？

![简化的照明](https://catlikecoding.com/unity/tutorials/rendering/part-2/default-scene/without-skybox.png)

背景颜色是每个相机定义的。它在默认情况下渲染天空盒，如果场景不存在天空盒，它也会默认设置为纯色。

![默认的相机设置](https://catlikecoding.com/unity/tutorials/rendering/part-2/default-scene/camera.png)

为了进一步简化渲染，可以禁用直射光对象，或者删除它。这将消除场景中的直接照明，以及由它所投射的阴影。剩下的是纯色的背景，以及被环境颜色包围着的球体的剪影。

![没有光照](https://catlikecoding.com/unity/tutorials/rendering/part-2/default-scene/without-light.png)

## 从对象到图像

我们这个非常简单的场景的绘制分为两个步骤。首先，用相机的背景色填充图像，然后在上面绘制出球体的轮廓。
Unity怎么知道它必须绘制一个球体呢？我们有一个球体对象，这个对象有一个mesh renderer组件。如果该对象位于摄像机的视野中，则应该渲染它。Unity通过检查对象的边界框是否与相机的视锥体相交来验证这一点。

![默认球体对象](https://catlikecoding.com/unity/tutorials/rendering/part-2/from-object-to-image/sphere-object.png)

transform组件用于改变网格和边界框的位置、方向和尺寸。实际上，使用了整个变换层次结构，如第1部分”矩阵“中所述。如果该对象最终出现在摄像机的视图中，则将其安排渲染。
最后，GPU的任务是渲染对象的网格。具体的渲染指令由对象的材质定义。材质引用了一个着色器——一个GPU程序——加上它可能有的任何设置。

![各司其职](https://catlikecoding.com/unity/tutorials/rendering/part-2/from-object-to-image/cpu-gpu.png)

我们的对象目前有默认的材质，它使用Unity的标准着色器。我们将用我们自己的着色器替换它，我们将从头开始构建。

### 你的第一个着色器

通过Assets / Create / shader / Unlit Shader创建一个新的着色器，命名为类似于_My First Shader_。

![你的第一个着色器](https://catlikecoding.com/unity/tutorials/rendering/part-2/from-object-to-image/my-first-shader.png)

打开着色器文件并删除它的内容，这样我们就可以从头开始了。
着色器是用shader关键字定义的。它后面是一个字符串，描述了你可以用来选择这个着色器的着色器菜单项。它不需要匹配文件名。然后是带有着色器内容的块。

```c
Shader "Custom/My First Shader" {

}
```

保存文件。你会得到一个该着色器不被支持的警告，因为它没有subshader或fallback。因为它是空的。
虽然着色器目前是没有功能的，但我们已经可以把它分配给材质了。所以通过Assets / create / material创建一个新的材质，然后从材质菜单中选择我们的材质。

![assets](https://catlikecoding.com/unity/tutorials/rendering/part-2/from-object-to-image/my-material.png)

![material](https://catlikecoding.com/unity/tutorials/rendering/part-2/from-object-to-image/material-with-shader.png)

更改球体对象，让它使用我们自己的材质，而不是默认材质。球体变成洋红色。这是因为Unity将其切换到一个错误着色器，它使用这种颜色来吸引你的注意力。

![object](https://catlikecoding.com/unity/tutorials/rendering/part-2/from-object-to-image/object-with-material.png)

![sphere](https://catlikecoding.com/unity/tutorials/rendering/part-2/from-object-to-image/sphere-with-material.png)

着色器错误提到了subshader。你可以使用这个来将多个着色器变体组合在一起。这允许你为不同的构建平台或LOD提供不同的subshader。例如，你可以有一个桌面端的subshader和一个移动端的subshader。我们现在只需要一个subshader块。

```c
Shader "Custom/My First Shader" {

	SubShader {
		
	}
}
```

subshader必须包含至少一个pass。subshader的pass是对象实际被渲染的地方。我们将使用一个pass，但有可能有更多。拥有多个pass意味着对象被渲染多次，这是实现很多的效果所需要的。

```c
Shader "Custom/My First Shader" {

	SubShader {

		Pass {

		}
	}
}
```

我们球体现在可能变成白色，因为我们正在使用一个空的pass的默认效果。如果发生这种情况，这意味着我们不再有任何着色器错误。但是，你可能仍然会在控制台看到旧的错误。

![一个白色的球体](https://catlikecoding.com/unity/tutorials/rendering/part-2/from-object-to-image/sphere-with-pass.png)

### 着色器程序

现在是时候来编写我们自己的着色器程序了。我们使用Unity的着色器语言，它是HLSL和CG着色语言的变体。我们必须使用CGPROGRAM关键字来指示代码的开头，并且以ENDCG关键字结束。

```c
Pass {
			CGPROGRAM

			ENDCG
		}
	
```

着色器编译器现在显示我们的着色器没有顶点和片段着色器。着色器由两个程序组成。顶点着色器负责处理网格的顶点数据。这包括从对象空间到显示空间的变换，就像我们在第1部分”矩阵“中所做的那样。片段程序负责着色位于网格三角形内的单个像素。

![顶点和片段程序](https://catlikecoding.com/unity/tutorials/rendering/part-2/from-object-to-image/vertex-fragment-programs.png)

我们必须通过pragma指令告诉编译器使用哪些程序。

```c
CGPROGRAM

			#pragma vertex MyVertexProgram
			#pragma fragment MyFragmentProgram

			ENDCG
```

编译器再次报错，这一次是因为它找不到我们指定的程序。因为我们还没有定义它们。
顶点和片段程序作为方法编写的，很像C#，尽管它们通常被称为函数。让我们简单地创建两个具有适当名称的返回类型为void的方法。

```
CGPROGRAM

			#pragma vertex MyVertexProgram
			#pragma fragment MyFragmentProgram

			void MyVertexProgram () {

			}

			void MyFragmentProgram () {

			}

			ENDCG
```

此时着色器将编译，然后球体将消失，或者还是会报错。这取决于编辑器使用的渲染平台。如果你使用DIrect3D 9，你可能得到的是报错。

### 着色器编译

Unity的着色器编译器把我们的代码转换成不同的程序，这取决于目标平台。不同的平台需要不同的解决方案。例如，用于Windows的Direct3D，用于mac的OpenGL，，用于移动设备的OpenGL ES，等等。这里我们不是在处理单个编译器，而是多个编译器。
最终使用哪种编译器取决于目标平台是什么。由于这些编译器并不相同，因此每个平台的结果可能不同。例如，我们的空程序在OpenGL和Direct3D 11上工作得很好，但是在针对Direct3D 9时却不行。在编辑器中选择着色器并查看检视窗口。它显示了一些关于着色器的信息，包括当前的编译错误。还有一个已编译的代码条目，带有一个Compile and show code按钮和一个下拉菜单。如果你点击按钮，Unity将编译着色器并在编辑器中打开它的输出，这样你就可以检查生成的代码了。

![着色器检视面板，所有平台都有错误](https://catlikecoding.com/unity/tutorials/rendering/part-2/from-object-to-image/shader-inspector.png)

你可以通过下拉菜单选择手动编译着色器的平台。默认是编译编辑器使用的图形设备。你也可以为其他平台手动编译，包括你当前的构建平台，你拥有许可证的所有平台或自定义选择。这使你能够快速确保你的着色器在多个平台上编译，而不必进行完整的构建。

![选择OpenGL Core](https://catlikecoding.com/unity/tutorials/rendering/part-2/from-object-to-image/compilation-selection.png)

要编译所选的程序，请关闭弹出窗口并单击compile和show code按钮。单击弹出窗口内的show按钮将显示使用的着色器变体，这在现在是没有用的。
例如，下面是当我们的着色器为OpenGL Core编译的结果代码。

```c
Shader "Custom/My First Shader" {
SubShader { 
 Pass {
  GpuProgramID 16807
Program "vp" {
SubProgram "glcore " {
"#ifdef VERTEX
#version 150
#extension GL_ARB_explicit_attrib_location : require
#extension GL_ARB_shader_bit_encoding : enable
void main()
{
    return;
}
#endif
#ifdef FRAGMENT
#version 150
#extension GL_ARB_explicit_attrib_location : require
#extension GL_ARB_shader_bit_encoding : enable
void main()
{
    return;
}
#endif
"
}
}
Program "fp" {
SubProgram "glcore " {
"// shader disassembly not supported on glcore"
}
}
 }
}
}
```

生成的代码分为两块，vp和fp，主要用于顶点和片段程序。然而，在OpenGL 的情况下，两个程序都在vp块中结束。这两个main函数对应着两个空的方法。让我们只关注这些，忽略其他代码。

```c
#ifdef VERTEX
void main()
{
    return;
}
#endif
#ifdef FRAGMENT
void main()
{
    return;
}
#endif
```

这里是为Direct3D 11生成的代码，分解成感兴趣的部分。它看起来很不一样，但是很明显，代码并没有做多少事情。

```c
Program "vp" {
SubProgram "d3d11 " {
      vs_4_0
   0: ret 
}
}
Program "fp" {
SubProgram "d3d11 " {
      ps_4_0
   0: ret 
}
}
```

在我们编写程序的过程中，我经常会展示OpenGL Core和Ｄ3D 11的编译代码，这样你就可以理解底层发生了什么。

### 包含其它文件

为了产生一个有功能的着色器，你需要大量的样板代码。定义通用变量、函数和其他内容的代码。如果这是一个C#程序，我们会把代码放在其他类中。但是着色器没有类。它们只是一个包含所有代码的大文件，没有类或名称空间提供的分组。
幸运的是，我们可以将代码分成多个文件。你可以使用#include指令将不同文件的内容加载到当前文件中。一个典型的文件是UnityCG.cginc，让我们来试一下。

```c
CGPROGRAM

			#pragma vertex MyVertexProgram
			#pragma fragment MyFragmentProgram

			#include "UnityCG.cginc"

			void MyVertexProgram () {

			}

			void MyFragmentProgram () {

			}

			ENDCG
```

UnityCG.cginc是一个着色器包含文件，是与Unity捆绑一起的。它包含一些其他的基本文件，并包含一些通用的功能。

![包含文件的层次结构，始于UnityCG](https://catlikecoding.com/unity/tutorials/rendering/part-2/from-object-to-image/include-files.png)

*UnityShaderVariables.cginc* 定义了一大堆渲染所需的着色器变量，比如变换，相机和光数据。这些都是由Unity在需要时设置的。
*HLSLSupport.cginc* 设置了一些东西，因此无论你的目标平台是哪个平台，都可以使用相同的代码。因此，你不必担心使用特定于平台的数据类型等问题。
*UnityInstancing.cginc* 专门用于实例化支持，这是一种减少drawcall的特定渲染技术。虽然它不直接包含文件，但它依赖于UnityShaderVariables。
请注意，这些文件的内容将被有效地复制到你自己的文件中，会替换include指令。这发生在预处理步骤中，该步骤执行所有预处理指令。这些指令都是以#开头的语句，比如#include和#pragma。完成该步骤后，将再次处理代码，并实际编译它。

### 产生输出

为了渲染一些东西，我们的着色程序必须产生结果。顶点程序必须返回一个顶点的最终坐标。多少维度的坐标呢？4个，因为我们用的是4×4的变换矩阵，如第1部分[矩阵](https://catlikecoding.com/unity/tutorials/rendering/part-1)所述 。
将函数的类型从void更改为float4。float4就是四个浮点数的集合，现在只返回0。

```c
float4 MyVertexProgram () {
				return 0;
			}
```

我们现在得到一个关于丢失语义的错误。编译器看到我们返回了一个float4的集合，但是它不知道这些数据代表什么。所以它不知道GPU应该怎么处理它。我们必须对程序的输出非常具体化。
在本例中，我们尝试输出顶点的位置。我们必须通过将SV_POSITION语义附加到我们的方法来表明这一点。SV表示系统值（system value），POSITION表示最终顶点位置。

```c
float4 MyVertexProgram () : SV_POSITION {
				return 0;
			}
```

片段程序应该输出一个像素的RGBA颜色值。我们也可以使用float4，返回0将产生一个纯黑色。

```c
float4 MyFragmentProgram () {
				return 0;
			}
```

fragment程序也需要语义。在这种情况下，我们必须指出最后的颜色应该写在哪里。我们使用SV_TARGET，它是默认的着色器目标。这是帧缓冲区，它包含我们生成的图像。

```c
float4 MyFragmentProgram () : SV_TARGET {
				return 0;
			}
```

顶点程序的输出被用作片段程序的输入。这意味着fragment程序应该获得一个与顶点程序输出相匹配的参数。

```c
float4 MyFragmentProgram (float4 position) : SV_TARGET {
				return 0;
			}
```

我们给参数去什么名字并不重要，但是我们必须确保使用正确的语义。

```c
float4 MyFragmentProgram (
				float4 position : SV_POSITION
			) : SV_TARGET {
				return 0;
			}
```

我们的着色器再次编译没有错误，但球体已经消失掉了。这并不奇怪，因为我们将它的所有顶点折叠成了一个点。
如果你查看编译后的OpenGL Core程序，你将看到它们现在写入到输出值中。我们的单一的值确实被四分量向量所代替。

```c
#ifdef VERTEX
void main()
{
    gl_Position = vec4(0.0, 0.0, 0.0, 0.0);
    return;
}
#endif
#ifdef FRAGMENT
layout(location = 0) out vec4 SV_TARGET0;
void main()
{
    SV_TARGET0 = vec4(0.0, 0.0, 0.0, 0.0);
    return;
}
#endif
```

D3D程序也是如此，尽管语法不同。

```c
Program "vp" {
SubProgram "d3d11 " {
      vs_4_0
      dcl_output_siv o0.xyzw, position
   0: mov o0.xyzw, l(0,0,0,0)
   1: ret 
}
}
Program "fp" {
SubProgram "d3d11 " {
      ps_4_0
      dcl_output o0.xyzw
   0: mov o0.xyzw, l(0,0,0,0)
   1: ret 
}
}
```

### 变换顶点

为了得到我们的球体，我们的顶点程序必须产生一个正确的顶点位置。为此，我们需要知道顶点的对象空间位置。我们可以通过向函数中添加一个POSITION语义变量来访问它。然后位置将作为    
$$
 \left[
 \begin{matrix}
   x  \\
   y  \\
   z  \\
   1
  \end{matrix}
  \right] 
$$
格式的齐次坐标提供，因此它的类型是float4。
[markdown如何输入矩阵？](https://blog.csdn.net/qq_38228254/article/details/79469727)



```c
float4 MyVertexProgram (float4 position : POSITION) : SV_POSITION {
				return 0;
			}
```

让我们直接返回这个位置：

```c
float4 MyVertexProgram (float4 position : POSITION) : SV_POSITION {
				return position;
			}
```

编译后的顶点程序现在将有一个顶点输入，并将其复制到输出：

```c
in  vec4 in_POSITION0;
void main()
{
    gl_Position = in_POSITION0;
    return;
}
```

```c
Bind "vertex" Vertex
      vs_4_0
      dcl_input v0.xyzw
      dcl_output_siv o0.xyzw, position
   0: mov o0.xyzw, v0.xyzw
   1: ret
```

![原始顶点位置](https://catlikecoding.com/unity/tutorials/rendering/part-2/from-object-to-image/raw-position.png)

你会看到一个黑色的球体，但它将被扭曲。这是因为我们使用对象空间的位置，就把它们当成显示位置一样。因此，移动球体在视觉上不会由什么影响。
我们必须将原始顶点位置与模型-视图-投影矩阵相乘。这个矩阵将对象的transform与摄像机的transform和投影结合起来，就像我们在第1部分”矩阵“中所做的那样。
UnityShaderVariables中将4×4 MVP矩阵定义为UNITY_MATRIX_MVP。我们可以使用mul函数将其与顶点位置相乘。这将正确地把我们的球体投射到显示器上。你还可以移动、旋转和缩放它，图像将按照预期那样的改变。

```c
float4 MyVertexProgram (float4 position : POSITION) : SV_POSITION {
				return mul(UNITY_MATRIX_MVP, position);
			}
```

![正确的位置](https://catlikecoding.com/unity/tutorials/rendering/part-2/from-object-to-image/correct-position.png)

如果你检查OpenGL Core顶点程序，你会发现很多uniform变量突然出现了。即使它们不会被使用，也会被忽略掉，访问矩阵也会触发编译器来包含整个矩阵。
你会看到矩阵乘法，被编码成一堆乘法和加法。

```c
uniform 	vec4 _Time;
uniform 	vec4 _SinTime;
uniform 	vec4 _CosTime;
uniform 	vec4 unity_DeltaTime;
uniform 	vec3 _WorldSpaceCameraPos;
…
in  vec4 in_POSITION0;
vec4 t0;
void main()
{
    t0 = in_POSITION0.yyyy * glstate_matrix_mvp[1];
    t0 = glstate_matrix_mvp[0] * in_POSITION0.xxxx + t0;
    t0 = glstate_matrix_mvp[2] * in_POSITION0.zzzz + t0;
    gl_Position = glstate_matrix_mvp[3] * in_POSITION0.wwww + t0;
    return;
}
```

D3D11编译器不需要包含未使用的变量。它用mul和三条mad指令对矩阵乘法进行编码。mad指令表示一个乘法后面跟一个加法。

```c
Bind "vertex" Vertex
ConstBuffer "UnityPerDraw" 352
Matrix 0 [glstate_matrix_mvp]
BindCB  "UnityPerDraw" 0
      vs_4_0
      dcl_constantbuffer cb0[4], immediateIndexed
      dcl_input v0.xyzw
      dcl_output_siv o0.xyzw, position
      dcl_temps 1
   0: mul r0.xyzw, v0.yyyy, cb0[1].xyzw
   1: mad r0.xyzw, cb0[0].xyzw, v0.xxxx, r0.xyzw
   2: mad r0.xyzw, cb0[2].xyzw, v0.zzzz, r0.xyzw
   3: mad o0.xyzw, cb0[3].xyzw, v0.wwww, r0.xyzw
   4: ret
```

## 给像素赋颜色

现在我们得到了正确的形状，让我们添加一些颜色。最简单的方法是使用纯色，例如黄色。

```c
float4 MyFragmentProgram (
				float4 position : SV_POSITION
			) : SV_TARGET {
				return float4(1, 1, 0, 1);
			}
```

![黄色球体](https://catlikecoding.com/unity/tutorials/rendering/part-2/coloring-pixels/yellow-sphere.png)

当然，你并不总是需要黄色的对象。理想情况下，我们的着色器将支持任何颜色。然后你可以通过材质来配置需要使用的颜色。这是通过着色器的属性（Properties）完成的。

### 着色器属性

着色器属性在一个单独的块中声明。将它添加到着色器的顶部。

```c
Shader "Custom/My First Shader" {

	Properties {
	}

	SubShader {
		…
	}
}
```

在新的块中放入一个名为_Tint的属性。你可以给它起任何名字，但是习惯上是以下划线开头，然后是大写字母，然后是小写字母。其思想是，没有其他任何东西使用这个约定，它可以防止意外的重复名称。

```c
Properties {
		_Tint
	}
```

属性名后面必须跟一个字符串和一个类型，在括号中，就像你调用一个方法一样。该字符串用于在材质检视面板中标记属性。在本例中，类型是Color。

```c
Properties {
		_Tint ("Tint", Color)
	}
```

属性声明的最后一部分是默认值的赋值。把它设为白色。
我们的着色器属性现在应该出现在着色器检视面板的属性部分了。

![着色器属性](https://catlikecoding.com/unity/tutorials/rendering/part-2/coloring-pixels/shader-tint.png)

当你选择你的材质，你将看到新的Tint属性，设置为白色。你可以将它更改为你喜欢的任何颜色，例如绿色。

![材质属性](https://catlikecoding.com/unity/tutorials/rendering/part-2/coloring-pixels/material-tint.png)

### 访问属性

要实际使用这个属性，我们必须在着色器代码中添加一个变量。它的名称必须与属性名称完全匹配，因此它是_Tint。然后，我们可以简单地在片段程序中返回该变量。

```c
#include "UnityCG.cginc"

			float4 _Tint;

			float4 MyVertexProgram (float4 position : POSITION) : SV_POSITION {
				return mul(UNITY_MATRIX_MVP, position);
			}

			float4 MyFragmentProgram (
				float4 position : SV_POSITION
			) : SV_TARGET {
				return _Tint;
			}
```

注意，在使用变量之前必须定义它。虽然你可以毫无问题地更改C#中的字段和方法的顺序，但对于着色器则不是这样。编译器从上到下工作，它不会向前看的。
编译后的片段程序现在包含了Tint变量。

```c
uniform 	vec4 _Time;
uniform 	vec4 _SinTime;
uniform 	vec4 _CosTime;
uniform 	vec4 unity_DeltaTime;
uniform 	vec3 _WorldSpaceCameraPos;
…
uniform 	vec4 _Tint;
layout(location = 0) out vec4 SV_TARGET0;
void main()
{
    SV_TARGET0 = _Tint;
    return;
}
```

```
ConstBuffer "$Globals" 112
Vector 96 [_Tint]
BindCB  "$Globals" 0
      ps_4_0
      dcl_constantbuffer cb0[7], immediateIndexed
      dcl_output o0.xyzw
   0: mov o0.xyzw, cb0[6].xyzw
   1: ret
```

![绿色球体](https://catlikecoding.com/unity/tutorials/rendering/part-2/coloring-pixels/green-sphere.png)

### 从顶点到片段

到目前为止，我们给了所有的像素相同的颜色。但是这是相当受限的。通常，顶点数据起着很大的作用。例如，我们可以把位置理解为一种颜色。然而，变换后的位置不是很有用。让我们使用网格的本地位置作为颜色。我们如何将那些额外的数据从顶点程序传递到片段程序。
GPU通过栅格化三角形来创建图像。它取三个处理过的顶点并在它们之间插值。对于三角形所覆盖的每个像素，它通过传递插值的数据去调用fragment程序。

![插值顶点数据](https://catlikecoding.com/unity/tutorials/rendering/part-2/coloring-pixels/interpolation.png)

所以顶点程序的输出根本不是直接用作片段着色器的输入。插值的过程介于两者之间。这里SV_POSITION数据被插值，但其他数据也有同样可以插值。
要访问插值过的局部位置，请在fragment程序中添加一个参数。因为我们只需要X，Y和Z分量，所以我们可以使用float3。然后我们可以输出位置，就好像它是一个颜色一样。我们必须提供第四个颜色分量，它可以保持为1.

```c
float4 MyFragmentProgram (
				float4 position : SV_POSITION,
				float3 localPosition
			) : SV_TARGET {
				return float4(localPosition, 1);
			}
```

同样，我们必须使用语义来告诉编译器如何解释这些数据。我们将使用TEXCOORD0。

```c
float4 MyFragmentProgram (
				float4 position : SV_POSITION,
				float3 localPosition : TEXCOORD0
			) : SV_TARGET {
				return float4(localPosition, 1);
			}
```

编译后的片段着色器现在将使用插值的数据，而不是uniform的tint数据。

```c
in  vec3 vs_TEXCOORD0;
layout(location = 0) out vec4 SV_TARGET0;
void main()
{
    SV_TARGET0.xyz = vs_TEXCOORD0.xyz;
    SV_TARGET0.w = 1.0;
    return;
}
```

```c
   ps_4_0
      dcl_input_ps linear v0.xyz
      dcl_output o0.xyzw
   0: mov o0.xyz, v0.xyzx
   1: mov o0.w, l(1.000000)
   2: ret
```

当然，顶点程序必须输出局部位置数据，才能使我们可以通过向其添加一个具有相同TEXCOORD0语义的输出参数来做到这一点。顶点和片段函数的参数名不需要匹配。这都是关于语义的。

```c
float4 MyVertexProgram (
				float4 position : POSITION,
				out float3 localPosition : TEXCOORD0
			) : SV_POSITION {
				return mul(UNITY_MATRIX_MVP, position);
			}
```

要通过顶点程序传递数据请将X，Y和Z分量从position复制到localPosition

```c
float4 MyVertexProgram (
				float4 position : POSITION,
				out float3 localPosition : TEXCOORD0
			) : SV_POSITION {
				localPosition = position.xyz;
				return mul(UNITY_MATRIX_MVP, position);
			}
```

额外的顶点程序输出被包含在编译器着色器中，我们将看到球体被着色了。

```c
in  vec4 in_POSITION0;
out vec3 vs_TEXCOORD0;
vec4 t0;
void main()
{
    t0 = in_POSITION0.yyyy * glstate_matrix_mvp[1];
    t0 = glstate_matrix_mvp[0] * in_POSITION0.xxxx + t0;
    t0 = glstate_matrix_mvp[2] * in_POSITION0.zzzz + t0;
    gl_Position = glstate_matrix_mvp[3] * in_POSITION0.wwww + t0;
    vs_TEXCOORD0.xyz = in_POSITION0.xyz;
    return;
}
```

```c
Bind "vertex" Vertex
ConstBuffer "UnityPerDraw" 352
Matrix 0 [glstate_matrix_mvp]
BindCB  "UnityPerDraw" 0
      vs_4_0
      dcl_constantbuffer cb0[4], immediateIndexed
      dcl_input v0.xyzw
      dcl_output_siv o0.xyzw, position
      dcl_output o1.xyz
      dcl_temps 1
   0: mul r0.xyzw, v0.yyyy, cb0[1].xyzw
   1: mad r0.xyzw, cb0[0].xyzw, v0.xxxx, r0.xyzw
   2: mad r0.xyzw, cb0[2].xyzw, v0.zzzz, r0.xyzw
   3: mad o0.xyzw, cb0[3].xyzw, v0.wwww, r0.xyzw
   4: mov o1.xyz, v0.xyzx
   5: ret
```

![用颜色来阐述局部位置](https://catlikecoding.com/unity/tutorials/rendering/part-2/coloring-pixels/local-position.png)

### 使用结构体

你会觉得我们的程序的参数列表看起来很乱吗？当我们在它们之间传递越来越多的数据时，情况只会变得更糟。由于顶点输出应该与片段输入匹配，如果我们可以在一个地方定义参数列表，那就很方便了。幸运的是，我们可以做到这一点。
我们可以定义数据结构，它只是变量的集合，它只是变量的集合。它们类似于C#中的结构体，只是语法稍有不同。这是一个结构体，它定义了我们要插值的数据。注意分号在其定义之后。

```c
struct Interpolators {
				float4 position : SV_POSITION;
				float3 localPosition : TEXCOORD0;
			};
```

使用这种结构使我们的代码更加整洁。

```c
float4 _Tint;
			
			struct Interpolators {
				float4 position : SV_POSITION;
				float3 localPosition : TEXCOORD0;
			};

			Interpolators MyVertexProgram (float4 position : POSITION) {
				Interpolators i;
				i.localPosition = position.xyz;
				i.position = mul(UNITY_MATRIX_MVP, position);
				return i;
			}

			float4 MyFragmentProgram (Interpolators i) : SV_TARGET {
				return float4(i.localPosition, 1);
			}
```

### 调整颜色

因为负值的颜色会被限制到0，我们的球体会变得相当的暗。因为默认的球体半径是1/2，最终颜色通道在-1/2~1/2之间。我们想把它们移到0-1范围，我们可以通过在所有通道加1/2得到。

```c
return float4(i.localPosition + 0.5, 1);
```

![局部位置重新着色](https://catlikecoding.com/unity/tutorials/rendering/part-2/coloring-pixels/local-position-01.png)

我们也可以应用我们的颜色乘以一个因子去着色。

```c
return float4(i.localPosition + 0.5, 1) * _Tint;
```

```c
uniform 	vec4 _Tint;
in  vec3 vs_TEXCOORD0;
layout(location = 0) out vec4 SV_TARGET0;
vec4 t0;
void main()
{
    t0.xyz = vs_TEXCOORD0.xyz + vec3(0.5, 0.5, 0.5);
    t0.w = 1.0;
    SV_TARGET0 = t0 * _Tint;
    return;
}
```

```c
ConstBuffer "$Globals" 128
Vector 96 [_Tint]
BindCB  "$Globals" 0
      ps_4_0
      dcl_constantbuffer cb0[7], immediateIndexed
      dcl_input_ps linear v0.xyz
      dcl_output o0.xyzw
      dcl_temps 1
   0: add r0.xyz, v0.xyzx, l(0.500000, 0.500000, 0.500000, 0.000000)
   1: mov r0.w, l(1.000000)
   2: mul o0.xyzw, r0.xyzw, cb0[6].xyzw
   3: ret
```

![](https://catlikecoding.com/unity/tutorials/rendering/part-2/coloring-pixels/local-position-tinted.png)

## 纹理化

如果你想添加更多的细节和多样性到网格上，在不添加更多的三角形情况下，你可以使用纹理实现。然后将图像映射到网格三角形上。
纹理坐标用于控制投影。这些是二维坐标对，覆盖了一个单位正方形区域内的整个图像，而不考虑纹理的实际纵横比。横坐标称为U，纵坐标称为V，因此，它们通常被称为UV坐标。

![覆盖图像的UV坐标](https://catlikecoding.com/unity/tutorials/rendering/part-2/texturing/uv-rectangle.png)

U坐标从左到右递增。所以它是0在左边，1/2是一半，1在右边。V坐标也是一样的，垂直方向。它从下到上递增，除了Direct3D，它从上到下递增。但是你几乎不需要担心这种差异。

### 使用UV坐标

Unity的默认网格有适合纹理映射的UV坐标。顶点程序可以通过带有TEXCOORD0语义的参数访问它们。

```c
Interpolators MyVertexProgram (
				float4 position : POSITION,
				float2 uv : TEXCOORD0
			) {
				Interpolators i;
				i.localPosition = position.xyz;
				i.position = mul(UNITY_MATRIX_MVP, position);
				return i;
			}
```

我们的顶点程序现在使用多个输入参数。同样，我们可以使用一个结构来对它们进行分组。

```c
struct VertexData {
				float4 position : POSITION;
				float2 uv : TEXCOORD0;
			};
			
			Interpolators MyVertexProgram (VertexData v) {
				Interpolators i;
				i.localPosition = v.position.xyz;
				i.position = mul(UNITY_MATRIX_MVP, v.position);
				return i;
			}
			
```

让我们将UV坐标传递给fragment程序，替换local position。

```c
struct Interpolators {
				float4 position : SV_POSITION;
				float2 uv : TEXCOORD0;
//				float3 localPosition : TEXCOORD0;
			};

			Interpolators MyVertexProgram (VertedData v) {
				Interpolators i;
//				i.localPosition = v.position.xyz;
				i.position = mul(UNITY_MATRIX_MVP, v.position);
				i.uv = v.uv;
				return i;
			}
```

我们可以使UV坐标可见，就像局部位置一样，通过将它们解释为颜色通道。例如，U变成红色，V标称绿色，而蓝色总是1。

```c
float4 MyFragmentProgram (Interpolators i) : SV_TARGET {
				return float4(i.uv, 1, 1);
			}
```

你将看到编译后的顶点程序现在将UV坐标从顶点数据复制到插值器输出。

```
in  vec4 in_POSITION0;
in  vec2 in_TEXCOORD0;
out vec2 vs_TEXCOORD0;
vec4 t0;
void main()
{
    t0 = in_POSITION0.yyyy * glstate_matrix_mvp[1];
    t0 = glstate_matrix_mvp[0] * in_POSITION0.xxxx + t0;
    t0 = glstate_matrix_mvp[2] * in_POSITION0.zzzz + t0;
    gl_Position = glstate_matrix_mvp[3] * in_POSITION0.wwww + t0;
    vs_TEXCOORD0.xy = in_TEXCOORD0.xy;
    return;
}
```

```
Bind "vertex" Vertex
Bind "texcoord" TexCoord0
ConstBuffer "UnityPerDraw" 352
Matrix 0 [glstate_matrix_mvp]
BindCB  "UnityPerDraw" 0
      vs_4_0
      dcl_constantbuffer cb0[4], immediateIndexed
      dcl_input v0.xyzw
      dcl_input v1.xy
      dcl_output_siv o0.xyzw, position
      dcl_output o1.xy
      dcl_temps 1
   0: mul r0.xyzw, v0.yyyy, cb0[1].xyzw
   1: mad r0.xyzw, cb0[0].xyzw, v0.xxxx, r0.xyzw
   2: mad r0.xyzw, cb0[2].xyzw, v0.zzzz, r0.xyzw
   3: mad o0.xyzw, cb0[3].xyzw, v0.wwww, r0.xyzw
   4: mov o1.xy, v1.xyxx
   5: ret
```

Unity将UV坐标包裹在球体周围，在两极折叠图像的顶部和底部，你会看到一条缝从北极延伸到南极，在那里，图像的左右两边连接在一起。沿着这条缝，你会得到0和1的U坐标值。这是通过在seam上有重复的顶点来实现的，除了它们的U坐标外，其他的都是相同的。

![frontal](https://catlikecoding.com/unity/tutorials/rendering/part-2/texturing/uv-sphere.png)

![from above](https://catlikecoding.com/unity/tutorials/rendering/part-2/texturing/uv-sphere-above.png)

### 添加一个纹理

要添加纹理，你需要导入一个图像文件。这是我将用于测试目的的一张图。

![用于测试的纹理](https://catlikecoding.com/unity/tutorials/rendering/part-2/texturing/texture.png)

你可以通过将图像拖放到Project视图中来将其添加到项目中。你也可以通过_Asset / Import New Asset_ 菜单项导入。图像将以2D纹理的形式导入，使用默认导入设置就好了。

![settings](https://catlikecoding.com/unity/tutorials/rendering/part-2/texturing/texture-inspector.png)

![preview](https://catlikecoding.com/unity/tutorials/rendering/part-2/texturing/texture-preview.png)

要使用纹理，我们必须添加另一个着色器属性。常规的纹理属性的类型是2D，因为还有其他类型的纹理，这里就不深入去讲。默认值是一个引用Unity默认纹理的字符串，可以是“white”、“black”或“gray”。
按照惯例将主纹理命名为_MainTex，所以我们将使用它。这也使你能够使用Material.mainTexture属性方便地访问。如果需要，可以通过脚本访问它。

```
Properties {
		_Tint ("Tint", Color) = (1, 1, 1, 1)
		_MainTex ("Texture", 2D) = "white" {}
	}
```

现在我们通过拖拽或点击Select按钮来为材质指定纹理。

![为我们的材质分配纹理](https://catlikecoding.com/unity/tutorials/rendering/part-2/texturing/material-inspector.png)

我们可以使用类型为sampler2D的变量来访问着色器中的纹理。

```
float4 _Tint;
			sampler2D _MainTex;
```

通过使用tex2D函数，在片段程序中使用UV坐标对纹理进行采样。

```c
float4 MyFragmentProgram (Interpolators i) : SV_TARGET {
				return tex2D(_MainTex, i.uv);
			}
```

```
uniform  sampler2D _MainTex;
in  vec2 vs_TEXCOORD0;
layout(location = 0) out vec4 SV_TARGET0;
void main()
{
    SV_TARGET0 = texture(_MainTex, vs_TEXCOORD0.xy);
    return;
}
```

```
SetTexture 0 [_MainTex] 2D 0
      ps_4_0
      dcl_sampler s0, mode_default
      dcl_resource_texture2d (float,float,float,float) t0
      dcl_input_ps linear v0.xy
      dcl_output o0.xyzw
   0: sample o0.xyzw, v0.xyxx, t0.xyzw, s0
   1: ret
```

![frontal](https://catlikecoding.com/unity/tutorials/rendering/part-2/texturing/textured-sphere.png)

![from above](https://catlikecoding.com/unity/tutorials/rendering/part-2/texturing/textured-sphere-above.png)

现在纹理被采样为一个个片段，它将出现在球面上。它像预期的那样环绕着球体，但在两个极点附近会显得相当不稳定。为什么会这样呢？
纹理失真的发生是因为插值是跨三角形线性的**？？？**。Unity的球体在两极附近只有几个三角形，那里的UV坐标扭曲得最厉害。所以UV坐标从一个顶点到另一个顶点是非线性变化的，但是在顶点之间它们的变化是线性的。结果，纹理中的直线在三角形边界处突然改变方向。

![三角形线性插值](https://catlikecoding.com/unity/tutorials/rendering/part-2/texturing/textured-sphere-wireframe.png)

不同的网格具有不同的UV坐标，从而产生不同的映射。Unity的默认球体使用经纬度纹理映射，而网格是一个低面数的立方体球体。这对于测试来说已经足够了，但是你最好使用一个自定义的球体网格来获得更好的结果。
最后，我们可以考虑用tint来调整球体的整体纹理外观。

```
return tex2D(_MainTex, i.uv) * _Tint
```

![夹带了黄色的纹理](https://catlikecoding.com/unity/tutorials/rendering/part-2/texturing/textured-tinted.png)

### 平铺和偏移

在我们添加纹理属性到着色器后，材质检视面板并不只是添加一个纹理字段。它还增加了平铺和偏移控制。然而，改变这些2D向量目前没有效果。
这些额外的纹理数据存储在材质中，也可以被着色器访问。你可以通过一个与相关材质具有相同名称的变量，加上_ST后缀来完成此操作。这个变量的类型必须是float4。

```
			sampler2D _MainTex;
			float4 _MainTex_ST;
```

*_ST 是什么意思？*：_ST后缀表示缩放和平移，或者类似的意思。为什么不根据Tiling和Offset的首字母而用 __TO呢？因为Unity总是使用 _ST，因为需要向后兼容性要求它保持这种方式，即使术语可能已经改变了。

平铺向量用于缩放纹理，因此默认为（1，1）,它存储在变量的XY部分。要使用它，只需将它与UV坐标相乘。这可以在顶点着色器或片段着色器中完成。在顶点着色器中这样做是有意义的，所以我们只对每个顶点而不是每个片段执行乘法。

```
Interpolators MyVertexProgram (VertexData v) {
				Interpolators i;
				i.position = mul(UNITY_MATRIX_MVP, v.position);
				i.uv = v.uv * _MainTex_ST.xy;
				return i;
			}
```

偏移部分移动纹理并存储在变量的zw分量中。它是在缩放后加到UV上的。

```
i.uv = v.uv * _MainTex_ST.xy + _MainTex_ST.zw;
```

UnityCG.cginc包含了一个方便的宏，它为我们简化了这个样板文件。我们可以把它当作方便的速记法。

```
i.uv = TRANSFORM_TEX(v.uv, _MainTex);
```

> 什么是宏？ ：宏类似于函数，只是它是在预处理（在真正编译之前）的时候执行的。这允许对代码执行文本操作，比如将 _ST附加到变量名。 TRANSFORM_TEX宏使用了这个技巧。如果你对这个很好奇，下面是它的定义 ：
>
> ```c
> #define TRANSFORM_TEX(tex,name) (tex.xy * name##_ST.xy + name##_ST.zw)
> ```
>
> 宏支持各种巧妙的技巧，但也可能导致难以理解的代码和非常讨厌的bug。这就是为什么C#没有宏的原因。
>
> 我们将在以后的教程中创建自己的宏。



## 纹理设置

到目前为止，我们已经使用了默认的纹理导入设置。让我们来看看一些选项，看看它们是什么意思。

![默认导入设置](https://catlikecoding.com/unity/tutorials/rendering/part-2/texturing/texture-inspector.png)

Wrap Mode指示使用0-1范围之外的UV坐标进行采样时会发生什么。当wrap mode被设置为clamped，UV被限制在0-1范围内。这意味着超出边缘的像素与位于边缘的像素相同。当wrap mode设置为repeat时，UV环绕覆盖。这意味着在边缘之外得像素和纹理得另一边得像素是相同的。默认模式是repeat，这会导致平铺效果。
如果你没有一个平铺的纹理，你把纹理的wrap mode设置为clamp。这可以防止纹理重复，相反，纹理边界将被复制，使它看起来被拉长了。

![设置为clamped，平铺为（2，2）](https://catlikecoding.com/unity/tutorials/rendering/part-2/texture-settings/clamp.png)

### Mipmaps和滤波

当纹理的像素（texel）与它们映射到的像素不完全匹配时会发生什么？这会导致匹配不当，必须要以某种方式解决。如何做到这一点时由Filter Mode决定的。
最直接的滤波模式是Point（没有过滤）。这意味着当纹理在某些UV坐标下采样时，会使用最近的像素。这将给纹理一个块状的外观，除非纹理映射精确到显示像素。因此，它通常用于像素完美的渲染，或需要块状样式时。
默认是使用bilinear滤波（双线性滤波）。当一个纹理在两个像素之间采样时，这两个像素会被插值。由于纹理是2D的，这发生在U轴和V轴上。因此是双线性滤波，而不仅仅是线性滤波。
这种方法在像素密度小于显示密度时有效，所以当你放大纹理时。结果会看起来很模糊。它在相反的情况下不起作用，当你缩小纹理的时候。相邻的显示像素将以间隔一个以上的像素的样本结束。这意味着部分纹理将被跳过，这将导致粗糙的过渡，就像图像被锐化了一样。
解决这个问题的方法是当纹理密度过高时使用较小的纹理。显示的纹理越小，应该使用更小的纹理。这些较小的纹理被称为mipmaps，是自动生成的。每个连续的mipmaps的宽度和高度都是前一级的一半。因此，当原始纹理大小为512×512时，mipmaps为256×256、128×128、64×64、32×32、16×16、8×8、4×4和2×2.
如果你想的话，可以禁用mipmaps。首先，将纹理类型设置为Advanced。你可以禁用mipmaps并应用更改。有一个很好的方法可以看出它们的区别，那就是使用一个Plane物体，比如一个四方平面。

![with](https://catlikecoding.com/unity/tutorials/rendering/part-2/texture-settings/bilinear-mips.png)

![without](https://catlikecoding.com/unity/tutorials/rendering/part-2/texture-settings/no-mips.png)

那么，在哪里使用哪个mipmap级别，它们看起来有什么不同？我们可以通过在Advanced纹理设置中启用Fadeout Mip Maps来让变换是可见的。当启用时，一个Fade Range 滑块将显示在检视面板中。它定义了一个mipmap范围，在这个范围内mipmap将转换为纯灰色。通过简单的一步过渡，你会得到一个清晰的灰色过渡。向右移动一步范围越远，转换就会发生得越晚。

![mipmaps得高级设置](https://catlikecoding.com/unity/tutorials/rendering/part-2/texture-settings/advanced-inspector.png)

为了更好的看到这个效果，把纹理的Aniso设置为0。

![mip 3](https://catlikecoding.com/unity/tutorials/rendering/part-2/texture-settings/mip-3.png)

![mip 4](https://catlikecoding.com/unity/tutorials/rendering/part-2/texture-settings/mip-4.png)

![mip 5](https://catlikecoding.com/unity/tutorials/rendering/part-2/texture-settings/mip-5.png)

一旦你知道了不同的mipmaps级别在哪里，你就可以看到它们之间纹理质量的突然变化。当纹理投影变小时，纹理密度增加，使其看起来更清晰。直到下一个mipmap级别突然出现，它再次变得模糊。
没有mipmaps时，图像就会从模糊到清晰，再过于清晰。使用mipmaps，你可以从模糊到清晰，再到突然模糊，再到锐化，再到突然模糊，等等。
这些锐化模糊带是双线性滤波的特征。你可以通过把过滤模式切换到三线性来摆脱它们。它的工作原理与双线性滤波相同，但它也可以在相邻的mipmap级别之间插入。因此是三线性的。这使得采样更加耗性能，但是它平滑了mipmap级别之间转换。

![三线性滤波之间得正常和灰色mipmaps](https://catlikecoding.com/unity/tutorials/rendering/part-2/texture-settings/trilinear-mip.png)

另一个有用得技术是各向异性滤波。你可能已经注意到，当你将其设置为0时，纹理变得更加模糊。这与mipmap级别的选择有关。
选择哪个mipmap级别取决于最坏的维度。如果差异很大，那么你将得到一个在一个维度中非常模糊得结果。各向异性滤波通过对维数的解耦来减轻这种影响。除了均匀地缩小纹理外，它还提供了在任意维度上缩放不同数量得版本。所以你不仅有一个256×256得mipmap，还有256×128，256×64等等。

![without](https://catlikecoding.com/unity/tutorials/rendering/part-2/texture-settings/without-aniso.png)

![with](https://catlikecoding.com/unity/tutorials/rendering/part-2/texture-settings/with-aniso.png)

注意，这些额外的mipmaps不像常规的mipmaps那样预先生成。相反，它们通过执行额外的纹理样本来模拟。所以他们不需要更多的空间，但需要更昂贵的采样成本。

![各向异性双线性滤波，过渡到灰色](https://catlikecoding.com/unity/tutorials/rendering/part-2/texture-settings/aniso-mip.png)

各项异性滤波的深度由Aniso级别控制。在0处，它是禁用的。在1时，它被启用并提供最小的效果。在16时，它达到了最大值。但是，这些设置会受到项目质量设置的影响。
你可以通过_Edit / Project Settings / Quality_。你会在渲染部分发现一个各向异性纹理设置。

![渲染质量设置](https://catlikecoding.com/unity/tutorials/rendering/part-2/texture-settings/rendering-quality.png)

当各向异性纹理被禁用时，无论纹理的设置如何，都不会发生各向异性滤波。当它被设置为每个纹理，它是完全由每个单独的纹理控制。它也可以设置为force On，就像每个纹理的Aniso级别设置为至少为9一样。但是，将Aniso级别设置为0的纹理仍然不会使用各向异性滤波。

下一篇教程 [合并纹理 ](https://catlikecoding.com/unity/tutorials/rendering/part-3/)

